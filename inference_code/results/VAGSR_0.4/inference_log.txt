INFO 05-09 04:41:58 [__init__.py:256] Automatically detected platform cuda.
INFO 05-09 04:42:01 [config.py:540] Found sentence-transformers tokenize configuration.
INFO 05-09 04:42:10 [config.py:436] Found sentence-transformers modules configuration.
INFO 05-09 04:42:10 [config.py:456] Found pooling configuration.
WARNING 05-09 04:42:10 [arg_utils.py:1754] --task embed is not supported by the V1 Engine. Falling back to V0. 
INFO 05-09 04:42:10 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.0) with config: model='Salesforce/SFR-Embedding-Mistral', speculative_config=None, tokenizer='Salesforce/SFR-Embedding-Mistral', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=Salesforce/SFR-Embedding-Mistral, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=PoolerConfig(pooling_type='LAST', normalize=False, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-09 04:42:12 [cuda.py:285] Using Flash Attention backend.
INFO 05-09 04:42:13 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 05-09 04:42:13 [model_runner.py:1110] Starting to load model Salesforce/SFR-Embedding-Mistral...
INFO 05-09 04:42:14 [weight_utils.py:257] Using model weights format ['*.safetensors']
INFO 05-09 04:43:49 [loader.py:429] Loading weights took 94.58 seconds
INFO 05-09 04:43:50 [model_runner.py:1146] Model loading took 13.2525 GB and 96.667654 seconds
